{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://export.arxiv.org/api/query?search_query=au:robert+koenig&cat:quant-ph&start=0&max_results=59'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    soup = BeautifulSoup(response, \"lxml\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Locking of accessible information and implications for the security of\\n  quantum cryptography'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = soup.find_all('title')\n",
    "titles = list(map(lambda x:x.text, titles))\n",
    "del titles[0]\n",
    "\n",
    "titles[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  The unconditional security of a quantum key distribution protocol is often\\ndefined in terms of the accessible information, that is, the maximum mutual\\ninformation between the distributed key S and the outcome of an optimal\\nmeasurement on the adversary's (quantum) system. We show that, even if this\\nquantity is small, certain parts of the key S might still be completely\\ninsecure when S is used in applications, such as for one-time pad encryption.\\nThis flaw is due to a locking property of the accessible information: one\\nadditional (physical) bit of information might increase the accessible\\ninformation by more than one bit.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = soup.find_all('summary')\n",
    "summaries = list(map(lambda x:x.text, summaries))\n",
    "summaries[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 15]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_name(name):\n",
    "    split_name = name.split(' ', 1)\n",
    "    return True if split_name[0] == \"Robert\" or split_name[1] == \"Koenig\" else False\n",
    "\n",
    "names = soup.find_all('name')\n",
    "names = list(map(lambda x:x.text, names))\n",
    "names = list(filter(retrieve_name, names))\n",
    "del names[names.index('Robert Howe')]\n",
    "\n",
    "indices = list([names.index('Alexander Koenig'), names.index('Robert A. Bridges')])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_to_remove = list()\n",
    "summaries_to_remove = list()\n",
    "\n",
    "for index in indices:\n",
    "    titles_to_remove.append(titles[index])\n",
    "    summaries_to_remove.append(summaries[index])\n",
    "    \n",
    "for title, summary in zip(titles_to_remove, summaries_to_remove):\n",
    "    titles.remove(title)\n",
    "    summaries.remove(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "koenig_papers = list()\n",
    "for title, summary in zip(titles, summaries):\n",
    "    koenig_papers.append(title + '.' + '\\n\\n' + summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conditional entropy power inequality for Gaussian quantum states.\n",
      "\n",
      "  We propose a generalization of the quantum entropy power inequality involving\n",
      "conditional entropies. For the special case of Gaussian states, we give a proof\n",
      "based on perturbation theory for symplectic spectra. We discuss some\n",
      "implications for entanglement-assisted classical communication over additive\n",
      "bosonic noise channels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(koenig_papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(koenig_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(query = \"quantum\", max_results=350, sort_by=arxiv.SortCriterion.SubmittedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = list()\n",
    "for result in search.results():\n",
    "    papers.append(result.title + '.' + '\\n\\n' + result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Encoding For Healthcare Data Democratisation and Information Leakage Prevention.\n",
      "\n",
      "The lack of data democratization and information leakage from trained models\n",
      "hinder the development and acceptance of robust deep learning-based healthcare\n",
      "solutions. This paper argues that irreversible data encoding can provide an\n",
      "effective solution to achieve data democratization without violating the\n",
      "privacy constraints imposed on healthcare data and clinical models. An ideal\n",
      "encoding framework transforms the data into a new space where it is\n",
      "imperceptible to a manual or computational inspection. However, encoded data\n",
      "should preserve the semantics of the original data such that deep learning\n",
      "models can be trained effectively. This paper hypothesizes the characteristics\n",
      "of the desired encoding framework and then exploits random projections and\n",
      "random quantum encoding to realize this framework for dense and longitudinal or\n",
      "time-series data. Experimental evaluation highlights that models trained on\n",
      "encoded time-series data effectively uphold the information bottleneck\n",
      "principle and hence, exhibit lesser information leakage from trained models.\n"
     ]
    }
   ],
   "source": [
    "print(papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text into sentences\n",
    "tokenized_papers = [nltk.sent_tokenize(paper) for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Encoding For Healthcare Data Democratisation and Information Leakage Prevention.',\n",
       " 'The lack of data democratization and information leakage from trained models\\nhinder the development and acceptance of robust deep learning-based healthcare\\nsolutions.',\n",
       " 'This paper argues that irreversible data encoding can provide an\\neffective solution to achieve data democratization without violating the\\nprivacy constraints imposed on healthcare data and clinical models.',\n",
       " 'An ideal\\nencoding framework transforms the data into a new space where it is\\nimperceptible to a manual or computational inspection.',\n",
       " 'However, encoded data\\nshould preserve the semantics of the original data such that deep learning\\nmodels can be trained effectively.',\n",
       " 'This paper hypothesizes the characteristics\\nof the desired encoding framework and then exploits random projections and\\nrandom quantum encoding to realize this framework for dense and longitudinal or\\ntime-series data.',\n",
       " 'Experimental evaluation highlights that models trained on\\nencoded time-series data effectively uphold the information bottleneck\\nprinciple and hence, exhibit lesser information leakage from trained models.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.argmin([len(paper) for paper in tokenized_papers])\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_papers[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Operads, homotopy theory and higher categories in algebraic quantum field theory.',\n",
       " 'This chapter provides a non-technical overview and motivation for the recent\\ninteractions between algebraic quantum field theory (AQFT) and rather abstract\\nmathematical disciplines such as operads, model categories and higher\\ncategories.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_papers[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of sentences in each paper\n",
    "num_sentences = [len(paper) for paper in tokenized_papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove papers with less than 2 sentences in the abstract\n",
    "valid_papers = list()\n",
    "for i in range(len(papers)):\n",
    "    if num_sentences[i] > 4:\n",
    "        valid_papers.append(papers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_valid_papers = [nltk.sent_tokenize(paper) for paper in valid_papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "vidx = np.argmin([len(paper) for paper in tokenized_valid_papers])\n",
    "print(vidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_valid_papers[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity of spin configurations dynamics due to unitary evolution and periodic projective measurements.\n",
      "\n",
      "We study the Hamiltonian dynamics of a many-body quantum system subjected to\n",
      "periodic projective measurements which leads to probabilistic cellular automata\n",
      "dynamics. Given a sequence of measured values, we characterize their dynamics\n",
      "by performing a principal component analysis. The number of principal\n",
      "components required for an almost complete description of the system, which is\n",
      "a measure of complexity we refer to as PCA complexity, is studied as a function\n",
      "of the Hamiltonian parameters and measurement intervals. We consider different\n",
      "Hamiltonians that describe interacting, non-interacting, integrable, and\n",
      "non-integrable systems, including random local Hamiltonians and translational\n",
      "invariant random local Hamiltonians. In all these scenarios, we find that the\n",
      "PCA complexity grows rapidly in time before approaching a plateau. The dynamics\n",
      "of the PCA complexity can vary quantitatively and qualitatively as a function\n",
      "of the Hamiltonian parameters and measurement protocol. Importantly, the\n",
      "dynamics of PCA complexity present behavior that is considerably less sensitive\n",
      "to the specific system parameters for models which lack simple local dynamics,\n",
      "as is often the case in non-integrable models. In particular, we point out a\n",
      "figure of merit that considers the local dynamics and the measurement direction\n",
      "to predict the sensitivity of the PCA complexity dynamics to the system\n",
      "parameters.\n"
     ]
    }
   ],
   "source": [
    "print(valid_papers[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = list()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "for token_paper in tokenized_valid_papers:\n",
    "    embeddings.append(model.encode(token_paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmin([embedding.shape[0] for embedding in embeddings])\n",
    "len(tokenized_valid_papers[38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06081802, -0.03395788,  0.02683055, ...,  0.02328086,\n",
       "        -0.09842872, -0.03384157],\n",
       "       [-0.03720855, -0.06228502, -0.00757967, ...,  0.06857048,\n",
       "         0.01825914, -0.03236686],\n",
       "       [-0.02817055, -0.06700587,  0.01448985, ...,  0.0744395 ,\n",
       "         0.01118425,  0.03867149]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the paper with the least number of sentences\n",
    "embeddings.pop(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin([embedding.shape[0] for embedding in embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[39].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all embeddings are not empty or have missing values\n",
    "for i in range(len(embeddings)):\n",
    "    if np.isnan(embeddings[i]).any():\n",
    "        print(i)\n",
    "    if len(embeddings[i]) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "c:\\Users\\marin\\anaconda3\\envs\\clust-arc\\lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "reducer = umap.UMAP(n_neighbors=4, init='random', n_components=2)\n",
    "umap_embeddings=list(map(lambda x:reducer.fit_transform(x), embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
